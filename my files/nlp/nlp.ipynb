{"cells":[{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1643492103596,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"R9go-Q4r2NYg"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19189,"status":"ok","timestamp":1643487486980,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"nZpD_Y5Q2NYn","outputId":"cbe74e57-f4b9-41e2-cb22-c602f14bccaa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import platform\n","\n","if platform.system() == 'Linux':\n","  from google.colab import drive\n","  drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"nz-38P0u2NYo"},"source":["#### The objective here is to one hot encode the words \n","\n","so for that we will asing a number to each character and then one hot encode that info in a array of the length of the num of characters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EeZRLb1F2NYs"},"outputs":[],"source":["if platform.system() == \"Linux\":\n","    with open ('/content/drive/MyDrive/deep learning/courses/pytorch_course//PYTORCH_NOTEBOOKS/PYTORCH_NOTEBOOKS/Data/shakespeare.txt', 'r', encoding='utf8') as f:\n","        text = f.read()\n","else:\n","    with open ('../../PYTORCH_NOTEBOOKS/PYTORCH_NOTEBOOKS/Data/shakespeare.txt', 'r', encoding='utf8') as f:\n","        text = f.read()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1643487488668,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"V90IWhyb2NYt","outputId":"8d6289db-ca9f-4c3c-8e64-6ad38333b719"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'str'>\n","5445609\n"]}],"source":["print(type(text))\n","print(len(text))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1643487488668,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"Ao8SGv1T2NYv","outputId":"0c710438-f613-4da3-d0db-09353aa9837a"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","                     1\n","  From fairest creatures we desire increase,\n","  That thereby beauty's rose might never die,\n","  But as the riper should by time decease,\n","  His tender heir might bear his memory:\n","  But thou contracted to thine own bright eyes,\n","  Feed'st thy light's flame with self-substantial fuel,\n","  Making a famine where abundance lies,\n","  Thy self thy foe, to thy sweet self too cruel:\n","  Thou that art now the world's fresh ornament,\n","  And only herald to the gaudy spring,\n","  Within thine own bud buriest thy content,\n","  And tender churl mak'st waste in niggarding:\n","    Pity the world, or else this glutton be,\n","    To eat the world's due, by the grave and thee.\n","\n","\n","                     2\n","  When forty winters shall besiege thy brow,\n","  And dig deep trenches in thy beauty's field,\n","  Thy youth's proud livery so gazed on now,\n","  Will be a tattered weed of small worth held:  \n","  Then being asked, where all thy beauty lies,\n","  Where all the treasure of thy lusty days;\n","  To say within thine own deep su\n"]}],"source":["print(text[:1000])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":525,"status":"ok","timestamp":1643487489185,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"hfETYozq2NYw","outputId":"165d629c-2026-4774-fd58-0f48a31edc1f"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'b', 'O', 'p', '}', '2', '?', '5', 'v', 'E', '`', 'A', '[', 'g', '(', 'j', '7', 'i', 'f', 'T', 'I', 'C', '|', 't', '1', 'k', '\\n', 'J', 's', '>', '-', 'L', '8', 'W', 'x', 'S', 'c', 'M', '\"', ':', 'P', '9', 'Q', 'N', '4', 'V', 'B', '_', 'e', 'F', 'z', 'Y', 'h', 'G', '3', 'X', 'l', 'Z', '6', ';', 'm', 'a', '0', 'q', 'u', 'd', 'n', 'o', \"'\", '<', 'D', 'y', '!', 'K', ']', 'w', 'R', ' ', 'U', '.', 'r', ')', ',', '&', 'H'} 84\n"]}],"source":["all_characters = set(text)\n","\n","print(all_characters, len(all_characters))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1643487489185,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"-1o_mkXA2NYy","outputId":"d63e782a-18b8-4a87-d613-689ed98ec022"},"outputs":[{"name":"stdout","output_type":"stream","text":["(0, 'b')\n","(1, 'O')\n","(2, 'p')\n","(3, '}')\n","(4, '2')\n","(5, '?')\n","(6, '5')\n","(7, 'v')\n","(8, 'E')\n","(9, '`')\n","(10, 'A')\n","(11, '[')\n","(12, 'g')\n","(13, '(')\n","(14, 'j')\n","(15, '7')\n","(16, 'i')\n","(17, 'f')\n","(18, 'T')\n","(19, 'I')\n","(20, 'C')\n","(21, '|')\n","(22, 't')\n","(23, '1')\n","(24, 'k')\n","(25, '\\n')\n","(26, 'J')\n","(27, 's')\n","(28, '>')\n","(29, '-')\n","(30, 'L')\n","(31, '8')\n","(32, 'W')\n","(33, 'x')\n","(34, 'S')\n","(35, 'c')\n","(36, 'M')\n","(37, '\"')\n","(38, ':')\n","(39, 'P')\n","(40, '9')\n","(41, 'Q')\n","(42, 'N')\n","(43, '4')\n","(44, 'V')\n","(45, 'B')\n","(46, '_')\n","(47, 'e')\n","(48, 'F')\n","(49, 'z')\n","(50, 'Y')\n","(51, 'h')\n","(52, 'G')\n","(53, '3')\n","(54, 'X')\n","(55, 'l')\n","(56, 'Z')\n","(57, '6')\n","(58, ';')\n","(59, 'm')\n","(60, 'a')\n","(61, '0')\n","(62, 'q')\n","(63, 'u')\n","(64, 'd')\n","(65, 'n')\n","(66, 'o')\n","(67, \"'\")\n","(68, '<')\n","(69, 'D')\n","(70, 'y')\n","(71, '!')\n","(72, 'K')\n","(73, ']')\n","(74, 'w')\n","(75, 'R')\n","(76, ' ')\n","(77, 'U')\n","(78, '.')\n","(79, 'r')\n","(80, ')')\n","(81, ',')\n","(82, '&')\n","(83, 'H')\n"]}],"source":["for pair in enumerate(all_characters):\n","    print(pair)\n"]},{"cell_type":"markdown","metadata":{"id":"fFwBYlSS2NY0"},"source":["#### The decoder transform from a num --> letter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7QG62h-2NY1"},"outputs":[],"source":["decoder = dict(enumerate(all_characters))"]},{"cell_type":"markdown","metadata":{"id":"AV1KAIrM2NY2"},"source":["### The encoder take a letter --> num"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1643487489186,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"9-bYfWzx2NY3","outputId":"3bbcb2b7-306e-4e30-c577-e80c9079c236"},"outputs":[{"data":{"text/plain":["dict_items([(0, 'b'), (1, 'O'), (2, 'p'), (3, '}'), (4, '2'), (5, '?'), (6, '5'), (7, 'v'), (8, 'E'), (9, '`'), (10, 'A'), (11, '['), (12, 'g'), (13, '('), (14, 'j'), (15, '7'), (16, 'i'), (17, 'f'), (18, 'T'), (19, 'I'), (20, 'C'), (21, '|'), (22, 't'), (23, '1'), (24, 'k'), (25, '\\n'), (26, 'J'), (27, 's'), (28, '>'), (29, '-'), (30, 'L'), (31, '8'), (32, 'W'), (33, 'x'), (34, 'S'), (35, 'c'), (36, 'M'), (37, '\"'), (38, ':'), (39, 'P'), (40, '9'), (41, 'Q'), (42, 'N'), (43, '4'), (44, 'V'), (45, 'B'), (46, '_'), (47, 'e'), (48, 'F'), (49, 'z'), (50, 'Y'), (51, 'h'), (52, 'G'), (53, '3'), (54, 'X'), (55, 'l'), (56, 'Z'), (57, '6'), (58, ';'), (59, 'm'), (60, 'a'), (61, '0'), (62, 'q'), (63, 'u'), (64, 'd'), (65, 'n'), (66, 'o'), (67, \"'\"), (68, '<'), (69, 'D'), (70, 'y'), (71, '!'), (72, 'K'), (73, ']'), (74, 'w'), (75, 'R'), (76, ' '), (77, 'U'), (78, '.'), (79, 'r'), (80, ')'), (81, ','), (82, '&'), (83, 'H')])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["decoder.items()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"70gWK16x2NY3"},"outputs":[],"source":["encoder = {char: ind for ind, char in decoder.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1643487489187,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"z92aFtZz2NY4","outputId":"1a0fecc3-ac10-4fae-ae39-3979d0aaaaed"},"outputs":[{"data":{"text/plain":["{'\\n': 25,\n"," ' ': 76,\n"," '!': 71,\n"," '\"': 37,\n"," '&': 82,\n"," \"'\": 67,\n"," '(': 13,\n"," ')': 80,\n"," ',': 81,\n"," '-': 29,\n"," '.': 78,\n"," '0': 61,\n"," '1': 23,\n"," '2': 4,\n"," '3': 53,\n"," '4': 43,\n"," '5': 6,\n"," '6': 57,\n"," '7': 15,\n"," '8': 31,\n"," '9': 40,\n"," ':': 38,\n"," ';': 58,\n"," '<': 68,\n"," '>': 28,\n"," '?': 5,\n"," 'A': 10,\n"," 'B': 45,\n"," 'C': 20,\n"," 'D': 69,\n"," 'E': 8,\n"," 'F': 48,\n"," 'G': 52,\n"," 'H': 83,\n"," 'I': 19,\n"," 'J': 26,\n"," 'K': 72,\n"," 'L': 30,\n"," 'M': 36,\n"," 'N': 42,\n"," 'O': 1,\n"," 'P': 39,\n"," 'Q': 41,\n"," 'R': 75,\n"," 'S': 34,\n"," 'T': 18,\n"," 'U': 77,\n"," 'V': 44,\n"," 'W': 32,\n"," 'X': 54,\n"," 'Y': 50,\n"," 'Z': 56,\n"," '[': 11,\n"," ']': 73,\n"," '_': 46,\n"," '`': 9,\n"," 'a': 60,\n"," 'b': 0,\n"," 'c': 35,\n"," 'd': 64,\n"," 'e': 47,\n"," 'f': 17,\n"," 'g': 12,\n"," 'h': 51,\n"," 'i': 16,\n"," 'j': 14,\n"," 'k': 24,\n"," 'l': 55,\n"," 'm': 59,\n"," 'n': 65,\n"," 'o': 66,\n"," 'p': 2,\n"," 'q': 62,\n"," 'r': 79,\n"," 's': 27,\n"," 't': 22,\n"," 'u': 63,\n"," 'v': 7,\n"," 'w': 74,\n"," 'x': 33,\n"," 'y': 70,\n"," 'z': 49,\n"," '|': 21,\n"," '}': 3}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1643487489187,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"WJVdnXag2NY4","outputId":"709f0b98-eaa7-48d8-b164-c3b0d9506a9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["60\n","a\n"]}],"source":["value = encoder['a']\n","print(value)\n","print(decoder[value])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hvKNT9g82NY5"},"outputs":[],"source":["encoded_text = np.array([encoder[char] for char in text])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KSUuqbvC2NY7"},"outputs":[],"source":["def one_hot_encoder(encoded_text, num_char):\n","    encoded = np.zeros((encoded_text.size, num_char))\n","    encoded = encoded.astype(np.float32)\n","    encoded[np.arange(encoded.shape[0]), encoded_text.flatten()] = 1.0\n","    encoded = encoded.reshape((*encoded_text.shape, num_char))\n","\n","    return encoded"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1643487491059,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"F6ODtvgd2NY7","outputId":"a75e2fd8-0402-4a9c-99af-49c6b60a6c67"},"outputs":[{"data":{"text/plain":["array([[1., 0., 0.],\n","       [0., 0., 1.],\n","       [0., 1., 0.]], dtype=float32)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["one_hot_encoder(np.array([0, 2, 1]), 3)"]},{"cell_type":"markdown","metadata":{"id":"YDQsSeZ12NY8"},"source":["### Now we are going to create all the batches\n","\n","This batches it consist basically that the words are shifted one for example if we have Hello how are you as the text we could do this\n","\n","h e l l o h o\n","\n","e l l o h o w\n","\n","And then we will continue doind this through the dataset\n","\n","X --> Encoded text of length seq_len\n","\n","Y --> encoded text shifted by one\n","\n","Plotting this shit basically the *samp_per_batch* is how many batches do you want and the *seq_len* is how many values do you have inside of each batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"06JV7Y-12NY8"},"outputs":[],"source":["def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n","\n","    # the num of characters that are in each batch\n","    char_per_batch = samp_per_batch * seq_len\n","\n","    # The num of abailable batches with the dataset\n","    num_batches_avail = int(len(encoded_text)/char_per_batch)\n","\n","    # drop out the values that don't fit inside of the batches\n","    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n","\n","    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n","    for n in range(0, encoded_text.shape[1], seq_len):\n","        x = encoded_text[:, n:n+seq_len]\n","        y = np.zeros_like(x)\n","\n","        try:\n","            y[:, :-1] = x[:, 1:]\n","            y[:, -1]  = encoded_text[:, n+seq_len]\n","\n","        except:\n","            y[:, :-1] = x[:, 1:]\n","            y[:, -1] = encoded_text[:, 0]\n","\n","        yield x, y"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1643487491060,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"WLb5DNif2NY9","outputId":"3f1fd6f2-cace-475e-bb8f-5f46d571de3a"},"outputs":[{"data":{"text/plain":["array([25, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76,\n","       76, 76, 76])"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["sample_text = encoded_text[:20]\n","sample_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WT-4qyoD2NY9"},"outputs":[],"source":["batch_generator = generate_batches(sample_text, 2, 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2XoG7D42NY-"},"outputs":[],"source":["x, y = next(batch_generator)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1643487491062,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"KRTIGbL62NY-","outputId":"4ac881c7-d67a-4232-87e0-513839c3a3bf"},"outputs":[{"data":{"text/plain":["array([[25, 76, 76, 76, 76],\n","       [76, 76, 76, 76, 76]])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1643487491062,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"uWemQ6_o2NY_","outputId":"fef65cde-97a5-4dae-a2d2-dc4873f05bb7"},"outputs":[{"data":{"text/plain":["array([[76, 76, 76, 76, 76],\n","       [76, 76, 76, 76, 76]])"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFAZKm3V2NY_"},"outputs":[],"source":["sample_nums = np.arange(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QR-Khgk12NY_"},"outputs":[],"source":["batch_generator = generate_batches(sample_nums, samp_per_batch=2, seq_len=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1643487491391,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"fcE-1gAz2NZA","outputId":"689a49b2-295e-483a-cbfc-97236b63fe40"},"outputs":[{"name":"stdout","output_type":"stream","text":["x: [[ 0  1  2  3  4]\n"," [10 11 12 13 14]]\n","\n","y: [[ 1  2  3  4  5]\n"," [11 12 13 14 15]]\n","\n","x: [[ 5  6  7  8  9]\n"," [15 16 17 18 19]]\n","\n","y: [[ 6  7  8  9  0]\n"," [16 17 18 19 10]]\n","\n"]}],"source":["for x, y in batch_generator:\n","    print(f'x: {x}')\n","    print()\n","    print(f'y: {y}')\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1643487491392,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"3HL-hWp_2NZA","outputId":"538877c9-9442-421f-996a-46c6e70bffed"},"outputs":[{"data":{"text/plain":["array([[ 6,  7,  8,  9,  0],\n","       [16, 17, 18, 19, 10]])"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["y"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1643487491393,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"6tsGxMiN2NZB","outputId":"d299259c-e700-4bab-e613-40cdfa41336b"},"outputs":[{"data":{"text/plain":["array([[ 6,  7,  8,  9,  0],\n","       [16, 17, 18, 19, 10]])"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["y"]},{"cell_type":"markdown","metadata":{"id":"0HOXhQ5b2NZC"},"source":["### Define the LSTM model\n","The hidden size it corresponds to these three values\n","\n","(**Number of layers**,                           **Batch size**,                  **Hidden size**)\n","\n","the number of layers of the lstm model | the size of the batches | the number of neurons that we have\n","\n","num_layers | batch_size | num_hidden"]},{"cell_type":"code","execution_count":57,"metadata":{"executionInfo":{"elapsed":678,"status":"ok","timestamp":1643492146709,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"R9MipRSg2NZD"},"outputs":[],"source":["class CharModel(nn.Module):\n","    def __init__(self, all_chars, num_hidden=256, num_layers=4, drop_prob=0.4, use_gpu=False):\n","        super().__init__()\n","        self.drop_prob = drop_prob\n","        self.num_layers = num_layers\n","        self.num_hidden = num_hidden\n","        self.use_gpu = use_gpu\n","\n","        self.all_chars = all_chars\n","        self.decoder = dict(enumerate(self.all_chars))\n","        self.encoder = {char:ind for ind, char in decoder.items()}\n","\n","        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n","        self.dropout = nn.Dropout(self.drop_prob)\n","        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n","\n","    def forward(self, x, hidden):\n","        lstm_output, hidden = self.lstm(x, hidden)\n","        drop_output = self.dropout(lstm_output)\n","        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n","        final_out = self.fc_linear(drop_output)\n","        return final_out, hidden\n","\n","    def hidden_state(self, batch_size):\n","        if self.use_gpu:\n","            hidden = (torch.zeros( self.num_layers, batch_size, self.num_hidden).cuda(),\n","                      torch.zeros( self.num_layers, batch_size, self.num_hidden).cuda())\n","        else:\n","            hidden = (torch.zeros( self.num_layers, batch_size, self.num_hidden),\n","                      torch.zeros( self.num_layers, batch_size, self.num_hidden))\n","        return hidden\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F26duBiQ2NZE"},"outputs":[],"source":["if torch.cuda.is_available():\n","    use_gpu = True\n","else:\n","    use_gpu = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FaaWZqUQ2NZE"},"outputs":[],"source":["model = CharModel(\n","    all_chars=all_characters,\n","    num_hidden=312,\n","    num_layers=3,\n","    drop_prob=0.5,\n","    use_gpu=use_gpu,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1643487491396,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"7L16SobE2NZF","outputId":"7f067f7c-a9ff-4db7-b09b-6f425e4a3173"},"outputs":[{"data":{"text/plain":["2085492"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["total_param = []\n","for p in model.parameters():\n","    total_param.append(int(p.numel()))\n","\n","sum(total_param)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"huxjERjf2NZG"},"outputs":[],"source":["optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_kSbZ1j82NZH"},"outputs":[],"source":["train_percent = 0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1z-y42w2NZH"},"outputs":[],"source":["train_ind = int(len(encoded_text) * (train_percent))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1643487491401,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"cPclB5hxIpqZ","outputId":"e6cdee33-511f-4b25-e773-e6a3bf331dda"},"outputs":[{"data":{"text/plain":["544560"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["train_ind"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TIsY_6NB2NZH"},"outputs":[],"source":["train_data = encoded_text[:train_ind]\n","val_data = encoded_text[train_ind:]"]},{"cell_type":"markdown","metadata":{"id":"6t7-6HcJ2NZM"},"source":["### Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1844387,"status":"ok","timestamp":1643489335768,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"rQ2KmjVW2NZN","outputId":"7e69c6b6-65dc-415d-e031-551531544764"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0 Step: 25 VAL LOSS: 3.246267795562744\n","Epoch: 1 Step: 50 VAL LOSS: 3.2347781658172607\n","Epoch: 1 Step: 75 VAL LOSS: 3.2337541580200195\n","Epoch: 2 Step: 100 VAL LOSS: 3.1972055435180664\n","Epoch: 2 Step: 125 VAL LOSS: 3.0936858654022217\n","Epoch: 3 Step: 150 VAL LOSS: 3.0141286849975586\n","Epoch: 4 Step: 175 VAL LOSS: 2.928138494491577\n","Epoch: 4 Step: 200 VAL LOSS: 2.8028173446655273\n","Epoch: 5 Step: 225 VAL LOSS: 2.746194839477539\n","Epoch: 5 Step: 250 VAL LOSS: 2.6873528957366943\n","Epoch: 6 Step: 275 VAL LOSS: 2.6004631519317627\n","Epoch: 7 Step: 300 VAL LOSS: 2.538698196411133\n","Epoch: 7 Step: 325 VAL LOSS: 2.4879000186920166\n","Epoch: 8 Step: 350 VAL LOSS: 2.4317188262939453\n","Epoch: 8 Step: 375 VAL LOSS: 2.3599636554718018\n","Epoch: 9 Step: 400 VAL LOSS: 2.3107879161834717\n","Epoch: 10 Step: 425 VAL LOSS: 2.2803025245666504\n","Epoch: 10 Step: 450 VAL LOSS: 2.2527055740356445\n","Epoch: 11 Step: 475 VAL LOSS: 2.223667860031128\n","Epoch: 11 Step: 500 VAL LOSS: 2.2003862857818604\n","Epoch: 12 Step: 525 VAL LOSS: 2.179187774658203\n","Epoch: 13 Step: 550 VAL LOSS: 2.1589488983154297\n","Epoch: 13 Step: 575 VAL LOSS: 2.133331775665283\n","Epoch: 14 Step: 600 VAL LOSS: 2.1203293800354004\n","Epoch: 14 Step: 625 VAL LOSS: 2.102815866470337\n","Epoch: 15 Step: 650 VAL LOSS: 2.086857318878174\n","Epoch: 16 Step: 675 VAL LOSS: 2.071881055831909\n","Epoch: 16 Step: 700 VAL LOSS: 2.0637967586517334\n","Epoch: 17 Step: 725 VAL LOSS: 2.047694206237793\n","Epoch: 17 Step: 750 VAL LOSS: 2.042842388153076\n","Epoch: 18 Step: 775 VAL LOSS: 2.0336432456970215\n","Epoch: 19 Step: 800 VAL LOSS: 2.0181703567504883\n","Epoch: 19 Step: 825 VAL LOSS: 2.0106842517852783\n","Epoch: 20 Step: 850 VAL LOSS: 1.999696969985962\n","Epoch: 20 Step: 875 VAL LOSS: 1.991455078125\n","Epoch: 21 Step: 900 VAL LOSS: 1.9797074794769287\n","Epoch: 22 Step: 925 VAL LOSS: 1.9724546670913696\n","Epoch: 22 Step: 950 VAL LOSS: 1.9671434164047241\n","Epoch: 23 Step: 975 VAL LOSS: 1.9554327726364136\n","Epoch: 23 Step: 1000 VAL LOSS: 1.95338773727417\n","Epoch: 24 Step: 1025 VAL LOSS: 1.9470515251159668\n","Epoch: 24 Step: 1050 VAL LOSS: 1.9345736503601074\n","Epoch: 25 Step: 1075 VAL LOSS: 1.9288029670715332\n","Epoch: 26 Step: 1100 VAL LOSS: 1.9261796474456787\n","Epoch: 26 Step: 1125 VAL LOSS: 1.9153993129730225\n","Epoch: 27 Step: 1150 VAL LOSS: 1.9081212282180786\n","Epoch: 27 Step: 1175 VAL LOSS: 1.907171607017517\n","Epoch: 28 Step: 1200 VAL LOSS: 1.9053274393081665\n","Epoch: 29 Step: 1225 VAL LOSS: 1.8927772045135498\n","Epoch: 29 Step: 1250 VAL LOSS: 1.8852843046188354\n","Epoch: 30 Step: 1275 VAL LOSS: 1.8852211236953735\n","Epoch: 30 Step: 1300 VAL LOSS: 1.8843787908554077\n","Epoch: 31 Step: 1325 VAL LOSS: 1.8734441995620728\n","Epoch: 32 Step: 1350 VAL LOSS: 1.867325782775879\n","Epoch: 32 Step: 1375 VAL LOSS: 1.8684604167938232\n","Epoch: 33 Step: 1400 VAL LOSS: 1.8596203327178955\n","Epoch: 33 Step: 1425 VAL LOSS: 1.859127163887024\n","Epoch: 34 Step: 1450 VAL LOSS: 1.8548800945281982\n","Epoch: 35 Step: 1475 VAL LOSS: 1.8565150499343872\n","Epoch: 35 Step: 1500 VAL LOSS: 1.846665859222412\n","Epoch: 36 Step: 1525 VAL LOSS: 1.8458387851715088\n","Epoch: 36 Step: 1550 VAL LOSS: 1.836257815361023\n","Epoch: 37 Step: 1575 VAL LOSS: 1.8381954431533813\n","Epoch: 38 Step: 1600 VAL LOSS: 1.8336575031280518\n","Epoch: 38 Step: 1625 VAL LOSS: 1.8306704759597778\n","Epoch: 39 Step: 1650 VAL LOSS: 1.8263248205184937\n","Epoch: 39 Step: 1675 VAL LOSS: 1.8215157985687256\n","Epoch: 40 Step: 1700 VAL LOSS: 1.8267368078231812\n","Epoch: 41 Step: 1725 VAL LOSS: 1.8217284679412842\n","Epoch: 41 Step: 1750 VAL LOSS: 1.8190815448760986\n","Epoch: 42 Step: 1775 VAL LOSS: 1.8157742023468018\n","Epoch: 42 Step: 1800 VAL LOSS: 1.809880018234253\n","Epoch: 43 Step: 1825 VAL LOSS: 1.8167535066604614\n","Epoch: 44 Step: 1850 VAL LOSS: 1.8092050552368164\n","Epoch: 44 Step: 1875 VAL LOSS: 1.810024380683899\n","Epoch: 45 Step: 1900 VAL LOSS: 1.8037550449371338\n","Epoch: 45 Step: 1925 VAL LOSS: 1.8006172180175781\n","Epoch: 46 Step: 1950 VAL LOSS: 1.8020362854003906\n","Epoch: 47 Step: 1975 VAL LOSS: 1.7975102663040161\n","Epoch: 47 Step: 2000 VAL LOSS: 1.796545147895813\n","Epoch: 48 Step: 2025 VAL LOSS: 1.797371506690979\n","Epoch: 48 Step: 2050 VAL LOSS: 1.7906092405319214\n","Epoch: 49 Step: 2075 VAL LOSS: 1.7920188903808594\n","Epoch: 49 Step: 2100 VAL LOSS: 1.7893397808074951\n","The training took 30.737232530117033 minutes\n"]}],"source":["import time\n","start_time = time.time()\n","\n","# Variables\n","epochs = 50\n","batch_size = 128\n","\n","seq_len = 100\n","tracker = 0\n","\n","num_char = max(encoded_text) + 1\n","\n","model.train()\n","\n","if model.use_gpu:\n","    model.cuda()\n","\n","\n","for i in range(epochs):\n","    hidden = model.hidden_state(batch_size)\n","\n","    for x, y in generate_batches(train_data, batch_size, seq_len):\n","        tracker += 1\n","\n","        x = one_hot_encoder(x, num_char)\n","        inputs = torch.from_numpy(x)\n","        targets = torch.from_numpy(y)\n","\n","        if model.use_gpu:\n","            inputs = inputs.cuda()\n","            targets = targets.cuda()\n","\n","        hidden = tuple([state.data for state in hidden])\n","        model.zero_grad()\n","        lstm_out, hidden = model.forward(inputs, hidden)\n","        loss = criterion(lstm_out,targets.view(batch_size*seq_len).long())\n","\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm = 5)\n","        optimizer.step()\n","\n","        if tracker % 25 == 0:\n","            val_hidden = model.hidden_state(batch_size = batch_size)\n","            val_losses = []\n","            model.eval()\n","\n","            for x, y in generate_batches(val_data, batch_size, seq_len):\n","                x = one_hot_encoder(x, num_char)\n","                inputs = torch.from_numpy(x)\n","                targets = torch.from_numpy(y)\n","\n","                if model.use_gpu:\n","                    inputs = inputs.cuda()\n","                    targets = targets.cuda()\n","\n","                val_hidden = tuple([state.data for state in hidden])\n","                lstm_out, hidden = model.forward(inputs, val_hidden)\n","                val_loss = criterion(lstm_out, targets.view(batch_size*seq_len).long())\n","\n","                val_losses.append(val_loss.item())\n","\n","            model.train()\n","            print(f\"Epoch: {i} Step: {tracker} VAL LOSS: {val_loss.item()}\")\n","\n","final_time = time.time() - start_time\n","print(f\"The training took {final_time/60} minutes\")\n","\n","# save of colab\n","if platform.system() == 'Linux':\n","  torch.save(model.state_dict(), '/content/drive/MyDrive/deep learning/courses/pytorch_course/my files/nlp/hidden512_layers3_shakes.pt')\n","else:\n","  torch.save(model.state_dict(), 'hidden512_layers3_shakes.pt')"]},{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":275,"status":"ok","timestamp":1643492154991,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"vboOIRPYtvEu"},"outputs":[],"source":["model = model = CharModel(\n","    all_chars=all_characters,\n","    num_hidden=312,\n","    num_layers=3,\n","    drop_prob=0.5,\n","    use_gpu=use_gpu,\n",")\n","\n","# load the model\n","if platform.system() == 'Linux':\n","  model.load_state_dict(torch.load('/content/drive/MyDrive/deep learning/courses/pytorch_course/my files/nlp/hidden512_layers3_shakes.pt'))\n","else:\n","  model.load_state_dict(torch.load('hidden512_layers3_shakes.pt', map_location=torch.device('cpu')))\n"]},{"cell_type":"markdown","metadata":{"id":"HB2DUAZdtvEv"},"source":["### Generate data using the model\n","\n","We will use 2 functions 1 that given 1 character predict other and other that uses the first one the generate all the text\n"]},{"cell_type":"markdown","metadata":{"id":"d5inQR1RtvEv"},"source":["The topk function basically consists in give the number of max values that you pass as k argument\n","\n","https://pytorch.org/docs/stable/generated/torch.topk.html\n","\n","The squeeze function deletes the leftovers axes of the array\n","\n","https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1643489335769,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"JD-Sa8KMtvEw","outputId":"f15a776c-9aba-4441-ded6-00aa3f3f837a"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.return_types.topk(\n","values=tensor([5., 4.]),\n","indices=tensor([4, 3]))\n"]},{"data":{"text/plain":["array(1)"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["# Example\n","\n","x = torch.arange(1., 6.)\n","x\n","print(torch.topk(x, 2))\n","\n","a = np.array([[[1]]]).squeeze()\n","a\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1643489335770,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"rhojSB-otvEw","outputId":"bdddebc9-e5b5-429d-8eb2-abb7009ffe3d"},"outputs":[{"data":{"text/plain":["3"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["np.random.choice([48, 5, 3], p=[.5, .25, .25])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eNHX5nbKtvEw"},"outputs":[],"source":["def predict_next_character(model, char, hidden=None, k=1):\n","    encoded_text = model.encoder[char]\n","    encoded_text = np.array([[encoded_text]])\n","    encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n","    inputs = torch.from_numpy(encoded_text)\n","\n","    if model.use_gpu:\n","        inputs = inputs.cuda()\n","\n","    hidden = tuple([state.data for state in hidden])\n","    lstm_out, hidden = model.forward(inputs, hidden)\n","    probs = F.softmax(lstm_out, dim = 1)\n","\n","    if model.use_gpu:\n","        probs = probs.cpu()\n","\n","    probs, index_positions = probs.topk(k)\n","    index_positions = index_positions.numpy().squeeze()\n","\n","    probs = probs.detach().numpy().flatten()\n","    probs = probs/probs.sum()\n","    char = np.random.choice(index_positions, p=probs)\n","\n","    return model.decoder[char], hidden"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s8XnMtThtvEy"},"outputs":[],"source":["def generate_text(model, size, seed='The', k=1):\n","    if model.use_gpu:\n","        model.cuda()\n","    else:\n","        model.cpu()\n","\n","    model.eval()\n","    output_chars = [c for c in seed]\n","    hidden = model.hidden_state(1)\n","\n","    for char in seed:\n","        char, hidden = predict_next_character(model, char, hidden, k=k)\n","\n","    output_chars.append(char)\n","\n","    for i in range(size):\n","        char, hidden = predict_next_character(model, output_chars[-1], hidden, k=k)\n","        output_chars.append(char)\n","\n","    return ''.join(output_chars)"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1524,"status":"ok","timestamp":1643492160053,"user":{"displayName":"zehcnas develop","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXpHWpTyLMNeMhgoaBW7Ndb2sDamFt52pjpRE5lA=s64","userId":"13138829643035246012"},"user_tz":-60},"id":"7gHXBt2StvEz","outputId":"0b874372-2bfa-453f-c868-2dc266943873"},"outputs":[{"output_type":"stream","name":"stdout","text":["The strong thee\n","    And the seening that the world thou he the sheet of the\n","    traest asself, the serpereds of the worthing a marder,\n","    That we seak make my lord of me to hear\n","    The world to he the wanter. I will too make\n","    Well the will and me that too his service,\n","    The serfort to this thou and make the with a fore thee\n","    stall to thy fore offing of thine, they have thing and shall see\n","    and the stares of thee anther and the seem of me.\n","    To the serves of the streansters.\n","  CLEOPATRA. That I say you, to madam, the praise of his\n","    strome that his statt of times the with him a serves,\n","    And the streeger of this wisting of mine ever of\n","    to marrer to the store the with to this so that to my.\n","  PAROLLES. Where is the selfed store the wite.\n","  ANTONY. I wall see the will the with a far thee are\n","    she he see her sto than the seems of this store.\n","\n","\n","                     23\n","  And the show all that the present the pountess,\n","  What where to the parting our see the stand,\n","  When\n"]}],"source":["print(generate_text(model, 1000, 'The ', k=3))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"nlp.ipynb","provenance":[]},"interpreter":{"hash":"42ee8ff86b4df163b477be19aa9b476089b78bae1aa05bfe38a352bb1be52ea6"},"kernelspec":{"display_name":"Python 3.7.3 64-bit ('dplr': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}